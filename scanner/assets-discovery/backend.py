from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import requests, urllib.parse, socket, concurrent.futures, time, re

app = Flask(__name__, template_folder="templates")
CORS(app)

HEADERS = {"User-Agent": "Delta-AssetDiscovery/1.0"}

# ---------- Helper functions ----------
def safe_get(url, params=None, headers=None, timeout=10):
    try:
        r = requests.get(url, params=params, headers=headers or HEADERS, timeout=timeout, verify=False) # Added verify=False for wider compatibility
        return r
    except Exception:
        return None

def normalize_domain(d):
    d = (d or "").strip().lower()
    d = re.sub(r"^https?://", "", d)
    d = d.split("/")[0]
    return d

def dedupe_keep_order(seq):
    seen = set()
    out = []
    for x in seq:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

# ---------- Core collectors ----------
def collect_crtsh_subdomains(domain):
    """crt.sh JSON endpoint (certificate transparency)."""
    subs = []
    try:
        url = f"https://crt.sh/?q=%25.{urllib.parse.quote(domain)}&output=json"
        r = safe_get(url, timeout=15)
        if r and r.status_code == 200:
            try:
                items = r.json()
                for it in items:
                    name = it.get("name_value") or it.get("common_name") or ""
                    for n in str(name).splitlines():
                        n = n.strip().lower()
                        if n and n.endswith(domain) and "*" not in n: # Filter out wildcards
                            subs.append(n)
            except Exception:
                pass
    except Exception:
        pass
    return dedupe_keep_order(subs)

def collect_bufferover_subdomains(domain):
    """
    dns.bufferover.run API
    Example: https://dns.bufferover.run/dns?q=example.com
    """
    subs = []
    try:
        url = "https://dns.bufferover.run/dns"
        params = {"q": domain}
        r = safe_get(url, params=params, timeout=10)
        if r and r.status_code == 200:
            j = r.json()
            # data.A (autogenerated) or FDNS_A
            for key in ("FDNS_A", "FDNS_CNAME", "RDNS", "Results"):
                if key in j and isinstance(j.get(key), list):
                    for item in j.get(key):
                        s = str(item)
                        # bufferover returns domain data in various forms; try to extract subdomains
                        if domain in s:
                            # extract token that looks like a hostname
                            m = re.findall(r"([a-zA-Z0-9._-]+\."+re.escape(domain)+")", s)
                            for sub in m:
                                subs.append(sub.lower())
            # sometimes "Results" contains hostnames directly
            if "Results" in j and isinstance(j["Results"], list):
                for ritem in j["Results"]:
                    if isinstance(ritem, str) and domain in ritem:
                        subs.append(ritem.lower())
    except Exception:
        pass
    return dedupe_keep_order(subs)

def collect_wayback_subdomains(domain):
    """Extract subdomains from Wayback links (CDX)"""
    subs = []
    try:
        cdx = f"https://web.archive.org/cdx/search/cdx?url=*.{urllib.parse.quote(domain)}/*&output=text&fl=original&collapse=urlkey"
        r = safe_get(cdx, timeout=15)
        if r and r.status_code == 200 and r.text:
            for line in r.text.splitlines():
                try:
                    host = urllib.parse.urlparse(line).hostname
                    if host and host.endswith(domain):
                        subs.append(host.lower())
                except Exception:
                    continue
    except Exception:
        pass
    return dedupe_keep_order(subs)

def collect_rapiddns_subdomains(domain):
    """
    RapidDNS scraping fallback. RapidDNS does not offer a guaranteed free JSON API.
    This function tries the public rapiddns endpoint that returns HTML (may change).
    If it fails, this returns empty list (non-fatal).
    """
    subs = []
    try:
        url = f"https://rapiddns.io/subdomain/{urllib.parse.quote(domain)}?full=1"
        r = safe_get(url, timeout=12)
        if r and r.status_code == 200 and r.text:
            # crude HTML parse: find hostnames in page
            matches = re.findall(r'([a-zA-Z0-9\-\_\.]+' + re.escape(domain) + r')', r.text)
            for m in matches:
                if m.endswith(domain):
                    subs.append(m.lower())
    except Exception:
        pass
    return dedupe_keep_order(subs)

# DNS records via Google DNS-over-HTTPS
def get_dns_records(domain):
    types = ["A", "AAAA", "MX", "NS", "TXT"]
    out = {}
    for t in types:
        try:
            url = "https://dns.google/resolve"
            params = {"name": domain, "type": t}
            r = safe_get(url, params=params, timeout=8)
            vals = []
            if r and r.status_code == 200:
                j = r.json()
                answers = j.get("Answer") or []
                for a in answers:
                    if isinstance(a, dict):
                        vals.append(a.get("data"))
                    else:
                        vals.append(a)
            out[t] = vals
        except Exception:
            out[t] = []
    return out

# Resolve A records for subdomains (collect IPs)
def resolve_ips_for_hosts(hosts):
    ips = set()
    def resolve_one(h):
        try:
            # try DNS via dns.google first
            url = "https://dns.google/resolve"
            params = {"name": h, "type": "A"}
            r = safe_get(url, params=params, timeout=6)
            if r and r.status_code == 200:
                j = r.json()
                for a in j.get("Answer") or []:
                    if isinstance(a, dict):
                        v = a.get("data")
                        if v and re.match(r"^\d+\.\d+\.\d+\.\d+$", v):
                            return v
            # as fallback use socket.gethostbyname (may block)
            return socket.gethostbyname(h)
        except Exception:
            return None
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        future_to_host = {executor.submit(resolve_one, h): h for h in hosts}
        for future in concurrent.futures.as_completed(future_to_host):
            ip = future.result()
            if ip:
                ips.add(ip)
                
    return sorted(list(ips))

# IP info via ip-api.com
def fetch_ip_info(ips):
    info = {}
    for ip in ips:
        try:
            r = safe_get(f"http://ip-api.com/json/{ip}", timeout=6)
            if r and r.status_code == 200:
                j = r.json()
                info[ip] = {
                    "country": j.get("country"),
                    "region": j.get("regionName"),
                    "city": j.get("city"),
                    "isp": j.get("isp"),
                    "org": j.get("org"),
                    "as": j.get("as"),
                }
            else:
                info[ip] = {}
        except Exception:
            info[ip] = {}
    return info

# Passive ports: lightweight HTTP probe against IPs on common ports
def probe_ports(ips, ports=[80, 443, 8080, 8000, 8443]):
    open_found = []
    def probe_one(ip, port):
        try:
            proto = "https" if port in (443, 8443) else "http"
            url = f"{proto}://{ip}:{port}/"
            r = safe_get(url, timeout=4)
            if r and 100 <= r.status_code < 600:
                return {"ip": ip, "port": port, "status": r.status_code}
        except Exception:
            return None
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as ex:
        futures = []
        for ip in ips:
            for p in ports:
                futures.append(ex.submit(probe_one, ip, p))
        for fut in concurrent.futures.as_completed(futures):
            res = fut.result()
            if res:
                open_found.append(res)
    return open_found

# Simple tech hints: headers + content checks
def detect_tech(hosts):
    tech = {}
    common_paths = ["/robots.txt", "/wp-login.php", "/admin", "/login", "/index.php"]
    for h in hosts[:30]:  # limit probing
        item = {"headers": {}, "hints": [], "found_paths": []}
        for scheme in ("https://", "http://"):
            try:
                url = f"{scheme}{h}"
                r = safe_get(url, timeout=6)
                if r:
                    # headers
                    if r.headers.get("Server"):
                        item["headers"]["server"] = r.headers.get("Server")
                    if r.headers.get("X-Powered-By"):
                        item["headers"]["x_powered_by"] = r.headers.get("X-Powered-By")
                    # content hints
                    text = r.text.lower()
                    if "wp-content" in text or "wp-includes" in text:
                        item["hints"].append("WordPress")
                    if "laravel" in text or "laravel" in r.headers.get("set-cookie","").lower():
                        item["hints"].append("Laravel")
                    if "django" in text:
                        item["hints"].append("Django")
                    if "react" in text or "create-react-app" in text:
                        item["hints"].append("React")
                    if "angular" in text:
                        item["hints"].append("Angular")
                    if "jquery" in text:
                        item["hints"].append("jQuery")
                    break
            except Exception:
                continue
        # probe common paths
        for p in common_paths:
            try:
                for scheme in ("https://", "http://"):
                    url = f"{scheme}{h}{p}"
                    r = safe_get(url, timeout=4)
                    if r and r.status_code < 400:
                        item["found_paths"].append({"path": p, "status": r.status_code})
                        break
            except Exception:
                continue
        tech[h] = item
    return tech

# Extract JS assets by fetching root page and finding .js links
def extract_js_assets(hosts, base_domain):
    js_assets = {}
    for h in hosts[:30]:
        assets = []
        for scheme in ("https://", "http://"):
            try:
                url = f"{scheme}{h}"
                r = safe_get(url, timeout=6)
                if r and r.text:
                    matches = re.findall(r'''(?i)(?:src=|href=)["']([^"']+\.js(?:\?[^"']*)?)["']''', r.text)
                    for m in matches:
                        # normalize
                        if m.startswith("//"):
                            m = "https:" + m
                        elif m.startswith("/"):
                            m = scheme + h + m
                        assets.append(m)
                    # also find api endpoints e.g., api.example.com
                    apis = re.findall(r'''(?i)(https?://[a-zA-Z0-9\-_\.]+\.[a-z]{2,6}/[a-zA-Z0-9_\-./?=,&%]+)''', r.text)
                    for a in apis:
                        if domain_in_string(a, base_domain): # Use base_domain for accuracy
                            assets.append(a)
                    break
            except Exception:
                continue
        js_assets[h] = dedupe_keep_order(assets)
    return js_assets

def domain_in_string(s, domain):
    return domain in s

# ---------- Flask routes ----------
@app.route("/")
def home():
    return render_template("asset_discovery.html")

@app.route("/api/asset", methods=["GET", "POST"]) # FIX: Allow both GET and POST requests
def api_asset():
    domain = ""
    vt_key = ""
    # FIX: Handle input from either a POST body or GET query parameter
    if request.method == "POST":
        payload = request.get_json() or {}
        domain = normalize_domain(payload.get("domain",""))
        vt_key = payload.get("vt_api_key","") or ""
    elif request.method == "GET":
        domain = normalize_domain(request.args.get("domain", ""))
        vt_key = request.args.get("vt_api_key","") or ""

    if not domain:
        return jsonify({"error":"domain required"}), 400

    result = {
        "domain": domain,
        "subdomains": [],
        "dns": {},
        "ips": [],
        "ip_info": {},
        "wayback_links": [],
        "tech": {},
        "open_ports": [],
        "js_assets": []
    }

    # 1) gather subdomains from multiple sources in parallel
    subs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as ex:
        futures = {
            ex.submit(collect_crtsh_subdomains, domain): "crtsh",
            ex.submit(collect_bufferover_subdomains, domain): "bufferover",
            ex.submit(collect_wayback_subdomains, domain): "wayback_subs",
            ex.submit(collect_rapiddns_subdomains, domain): "rapiddns"
        }
        for fut in concurrent.futures.as_completed(futures):
            try:
                vals = fut.result() or []
                subs.extend(vals)
            except Exception:
                pass

    result["subdomains"] = dedupe_keep_order([s for s in subs if s])

    # 2) DNS records for main domain
    result["dns"] = get_dns_records(domain)

    # 3) Wayback links (full URLs)
    try:
        cdx = f"https://web.archive.org/cdx/search/cdx?url=*.{urllib.parse.quote(domain)}/*&output=text&fl=original&collapse=urlkey"
        r = safe_get(cdx, timeout=15)
        if r and r.status_code == 200:
            result["wayback_links"] = [l for l in r.text.splitlines() if l.strip()]
    except Exception:
        result["wayback_links"] = []

    # 4) gather IPs: from DNS A records + resolve subdomains
    ips = set()
    for a in result["dns"].get("A",[]):
        # A records sometimes contain TTL or other; try to extract ip
        ip = a.split()[-1] if " " in a else a
        if re.match(r"^\d+\.\d+\.\d+\.\d+$", ip):
            ips.add(ip)
    # resolve subdomains
    resolved = resolve_ips_for_hosts(result["subdomains"])
    for ip in resolved:
        ips.add(ip)
    result["ips"] = sorted(list(ips))

    # 5) get ip info (ip-api)
    result["ip_info"] = fetch_ip_info(result["ips"])

    # 6) tech hints
    hosts_to_probe = [domain] + result["subdomains"]
    result["tech"] = detect_tech(hosts_to_probe)

    # 7) js assets
    result["js_assets"] = extract_js_assets(result["subdomains"] or [domain], domain)

    # 8) passive port probe
    result["open_ports"] = probe_ports(result["ips"])

    return jsonify(result), 200

if __name__ == "__main__":
    app.run(host="127.0.0.1", port=5000, debug=True)
